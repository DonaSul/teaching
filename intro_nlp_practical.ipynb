{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCUw45mZKjG"
      },
      "source": [
        "Intro to NLP Practical<br>\n",
        "======================<br>\n",
        "Students will work through problems on n-grams, probabilities, OOV handling, and classifiers.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hJnSQ_vqZKjO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2tr5e7WZKjQ"
      },
      "source": [
        "Toy corpus for language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0uJmCM0sZKjS"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"Its fleece was white as snow\",\n",
        "    \"And everywhere that Mary went\",\n",
        "    \"The lamb was sure to go\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIvCzEeZKjT"
      },
      "source": [
        "--- Part 1: Preprocessing ---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.1 Sequence notation\n",
        "Exercise: Write sequence notation for the sentence:\n",
        "\"Mary had a little lamb, its fleece was white as snow\""
      ],
      "metadata": {
        "id": "CArblc3XHU5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=[\"Mary had a little lamb, its fleece was white as snow\"]"
      ],
      "metadata": {
        "id": "oOsAJt_HHLTm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)  #specifies that the tokenizer will only consider the top 100 most frequent words in the corpus.\n",
        "\n",
        "tokenizer.fit_on_texts(sentence) # It essentially builds a word index where each unique word in the sentence is assigned an integer ID.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "print(\"Word Index: \", word_index)\n",
        "print(\"Sequences: \", sequences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xuxbnQaF8zs",
        "outputId": "32da9a4b-6f8d-4049-a29f-bb640b39a808"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:  {'mary': 1, 'had': 2, 'a': 3, 'little': 4, 'lamb': 5, 'its': 6, 'fleece': 7, 'was': 8, 'white': 9, 'as': 10, 'snow': 11}\n",
            "Sequences:  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAOpzABuZKjV"
      },
      "source": [
        " Q1.2 Add start/end tokens<br>\n",
        "Exercise: Write a function to tokenize the corpus and add <s>, </s>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_end_tokens(corpus):\n",
        "    start_token = \"<s>\"\n",
        "    end_token = \"</s>\"\n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = [start_token] + sentence.split() + [end_token]\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "    return tokenized_corpus\n",
        "\n",
        "tokenized_corpus = add_start_end_tokens(sentence)\n",
        "print(tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8lGovgpIBnU",
        "outputId": "098b13b3-b316-460d-a8dd-cb3e021ae0cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<s>', 'Mary', 'had', 'a', 'little', 'lamb,', 'its', 'fleece', 'was', 'white', 'as', 'snow', '</s>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH1mpV5ZKjY"
      },
      "source": [
        "--- Part 2: N-grams & Probabilities ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xVR2UlsZKjZ"
      },
      "source": [
        " Q2.1 Extract unigrams, bigrams, trigrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "unigrams = []\n",
        "bigrams = []\n",
        "trigrams = []\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    unigrams.extend(list(ngrams(sentence, 1)))\n",
        "    bigrams.extend(list(ngrams(sentence, 2)))\n",
        "    trigrams.extend(list(ngrams(sentence, 3)))\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSGy0_0ALUch",
        "outputId": "97dcc674-678d-480a-c146-57ab5ea23c20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: [('<s>',), ('Mary',), ('had',), ('a',), ('little',), ('lamb,',), ('its',), ('fleece',), ('was',), ('white',), ('as',), ('snow',), ('</s>',)]\n",
            "Bigrams: [('<s>', 'Mary'), ('Mary', 'had'), ('had', 'a'), ('a', 'little'), ('little', 'lamb,'), ('lamb,', 'its'), ('its', 'fleece'), ('fleece', 'was'), ('was', 'white'), ('white', 'as'), ('as', 'snow'), ('snow', '</s>')]\n",
            "Trigrams: [('<s>', 'Mary', 'had'), ('Mary', 'had', 'a'), ('had', 'a', 'little'), ('a', 'little', 'lamb,'), ('little', 'lamb,', 'its'), ('lamb,', 'its', 'fleece'), ('its', 'fleece', 'was'), ('fleece', 'was', 'white'), ('was', 'white', 'as'), ('white', 'as', 'snow'), ('as', 'snow', '</s>')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuQzfXWZKjb"
      },
      "source": [
        " Q2.2 Bigram probabilities<br>\n",
        "Exercise: Write function to compute P(w_i | w_{i-1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_probabilities(bigrams):\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    unigram_counts = Counter([bigram[0] for bigram in bigrams])\n",
        "    probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        word1, word2 = bigram\n",
        "        unigram_count = unigram_counts[word1]\n",
        "        # Calculate conditional probability\n",
        "        probability = count / unigram_count\n",
        "        probabilities[bigram] = probability\n",
        "\n",
        "    return probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "uzTxzUCINrGa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_probabilities = bigram_probabilities(bigrams)\n",
        "print(bigram_probabilities)"
      ],
      "metadata": {
        "id": "uiKa2vq8SI0C",
        "outputId": "d5005db6-bd94-4fc2-d55b-dceb5cfe9be5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('<s>', 'Mary'): 1.0, ('Mary', 'had'): 1.0, ('had', 'a'): 1.0, ('a', 'little'): 1.0, ('little', 'lamb,'): 1.0, ('lamb,', 'its'): 1.0, ('its', 'fleece'): 1.0, ('fleece', 'was'): 1.0, ('was', 'white'): 1.0, ('white', 'as'): 1.0, ('as', 'snow'): 1.0, ('snow', '</s>'): 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G1JGIAZKjj"
      },
      "source": [
        " Q2.3 Sentence probability<br>\n",
        "Exercise: Compute probability of \"Mary had a little lamb\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0unZilSZKjk"
      },
      "source": [
        "Q2.4 Handling OOV/UNK<br>\n",
        "Exercise: Replace unseen words with <UNK> and recompute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evU5bff_ZKjl"
      },
      "source": [
        "--- Part 3: Classifier ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DxxnAKZKjm"
      },
      "source": [
        " Q3.1 Naive Bayes sentiment classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìΩ Exercise 3.1: Sentiment Classification on toy dataset\n",
        "\n",
        "In this exercise, you will build a simple sentiment classification model that predicts whether a given sentence is **positive** or **negative**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úèÔ∏è Instructions:\n",
        "\n",
        "\n",
        "### 1Ô∏è‚É£ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "\n",
        "üöÄ **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ],
      "metadata": {
        "id": "Yyx3z1RL58el"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s3fkQ5DUZKjm"
      },
      "outputs": [],
      "source": [
        "train_texts = [\n",
        "    \"I love my dog\",\n",
        "    \"This food is great\",\n",
        "    \"I hate waiting\",\n",
        "    \"The movie was boring\",\n",
        "    \"Happy with my phone\",\n",
        "    \"This is awful\"\n",
        "]\n",
        "train_labels = [\"pos\", \"pos\", \"neg\", \"neg\", \"pos\", \"neg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìΩ Exercise 3.2: Movie Review Classification using Movies Review Corpus\n",
        "\n",
        "In this exercise, you will build a simple text classification model that predicts whether a given **movie review** is **positive** or **negative** using the **NLTK Movie Reviews Corpus**.\n",
        "\n",
        "This is a classical example of text classification at the **sentence level**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úèÔ∏è Instructions:\n",
        "\n",
        "### 1Ô∏è‚É£ Load the Data\n",
        "- Import the **Movie Reviews corpus** from **NLTK**.\n",
        "- Create a dataset where each example is a review and the label is either `'positive'` or `'negative'`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ Evaluate the Classifier\n",
        "- Use **accuracy** and a **classification report** to evaluate your model on the test set.\n",
        "- Think about: How well does the model perform? Which reviews are harder to classify?\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ You are free to explore:\n",
        "- Trying different classifiers.\n",
        "- Visualizing the results (e.g., confusion matrix).\n",
        "\n",
        "---\n",
        "\n",
        "üöÄ **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ],
      "metadata": {
        "id": "fq7Vj1ng1lw8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYgyaWtZKju"
      },
      "source": [
        " Q3.3 Discussion: Why bigrams vs unigrams?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv13VAlzZKju"
      },
      "source": [
        " Q3.4 Limitations of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mIB4S3wZKju"
      },
      "source": [
        "--- Part 4: Wrap-up Reflection ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzsTXZsTZKjv"
      },
      "source": [
        " Discussion Questions<br>\n",
        "1. Why do we need <UNK> tokens?<br>\n",
        "2. Why start/end tokens?<br>\n",
        "3. Why not always use higher n-grams?<br>\n",
        "4. How do classifiers differ from language models?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}