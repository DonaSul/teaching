{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonaSul/teaching/blob/main/intro_nlp_day_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXuvGMYIiVf8"
      },
      "source": [
        "\n",
        "<br>\n",
        "====================================================<br>\n",
        "RNN Improvements Practical<br>\n",
        "Vanishing/Exploding Gradients, GRUs and LSTMs<br>\n",
        "====================================================<br>\n",
        "Learning Goals:<br>\n",
        "- Understand vanishing and exploding gradients in RNNs<br>\n",
        "- Apply gradient clipping as a solution<br>\n",
        "- Implement GRU and LSTM for text classification<br>\n",
        "- Compare GRU vs LSTM on IMDB Sentiment Dataset<br>\n",
        "====================================================<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.1 torchtext==0.18.0 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_u4ArT5jIKV",
        "outputId": "3559c03d-c869-4ea4-a380-cf12ddea5777"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.3.1\n",
            "  Downloading https://download.pytorch.org/whl/rocm6.0/torch-2.3.1%2Brocm6.0-cp312-cp312-linux_x86_64.whl (2193.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m342.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.18.0\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchtext-0.18.0%2Bcpu-cp312-cp312-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.1+rocm6.0 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.1+rocm6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.3.1+rocm6.0 torchtext-0.18.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "luaWkqVQiVgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f631e4b-b59c-431e-c86f-25d4eca567b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-5_jal8iVgD"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 1 — Conceptual<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgmxt9v0iVgE"
      },
      "source": [
        "\n",
        "<br>\n",
        "Q1: Why do RNNs suffer from vanishing or exploding gradients?<br>\n",
        "Write your answer here:<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because during backpropagation through time, gradients are repeatedly multiplied across many timesteps. If values are < 1, they shrink exponentially (vanishing), and if > 1, they grow uncontrollably (exploding), making it hard to learn long-term dependencies or causing unstable training."
      ],
      "metadata": {
        "id": "cIiJHtyRk0N7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8HyRWoUiVgE"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 2 — Demonstrate Exploding Gradients <br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E7xWZiL4iVgF"
      },
      "outputs": [],
      "source": [
        "rnn = nn.RNN(input_size=1, hidden_size=1, batch_first=True) # Simple RNN with 1 input feature and 1 hidden unit\n",
        "\n",
        "'''\n",
        "x = torch.ones((1, 50, 1))  # long sequence of ones, (batch=1, seq_len=50, input_size=1)\n",
        "target = torch.tensor([1])  # fake label\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(rnn.parameters(), lr=1.0)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SceYkkH_iVgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cda6a0d-7bff-4c4c-8573-7a9c57604d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 2: Exploding Gradients Demonstration ---\n",
            "Epoch 1 | Loss: 9.974962e-01 | Gradient norm: 1.007493e-02\n",
            "Epoch 2 | Loss: 9.974861e-01 | Gradient norm: 1.011637e-02\n",
            "Epoch 3 | Loss: 9.974758e-01 | Gradient norm: 1.015781e-02\n",
            "Epoch 4 | Loss: 9.974654e-01 | Gradient norm: 1.019973e-02\n",
            "Epoch 5 | Loss: 9.974551e-01 | Gradient norm: 1.024165e-02\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Task 2: Exploding Gradients Demonstration ---\")\n",
        "#your code here\n",
        "\n",
        "with torch.no_grad():\n",
        "    rnn.weight_hh_l0.fill_(3.0)    # forcingg the recurrent weight\n",
        "    rnn.weight_ih_l0.fill_(1.0)\n",
        "\n",
        "x = torch.ones((1, 50, 1))\n",
        "target = torch.tensor([[0.0]])\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(rnn.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    out, _ = rnn(x)\n",
        "    y_pred = out[:, -1, :]\n",
        "\n",
        "    loss = criterion(y_pred, target)\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient norm\n",
        "    total_norm = 0\n",
        "    for p in rnn.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.6e} | Gradient norm: {total_norm:.6e}\")\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Gnorm, how big the gradients are across all the parameters of your model at a training step.\n",
        "    '''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones((1, 50, 1))\n",
        "print(x.shape)\n"
      ],
      "metadata": {
        "id": "mBC9D0Mywlt_",
        "outputId": "ddeaa37c-f32e-40f5-dece-a4fcfd676d1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXerLwLziVgG"
      },
      "source": [
        "\n",
        "<br>\n",
        "# --- TIP ---<br>\n",
        "You should observe gradient norms growing very large, signifying an exploding gradient problem. Why is that the case? How can you remedy that?<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26j9TnoniVgH"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 3 — Apply Gradient Clipping (15 mins)<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient clipping is a technique to prevent exploding gradients by forcing the gradient norm to stay below a threshold before doing the parameter update."
      ],
      "metadata": {
        "id": "Ac8JrWQb0WSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BAyRJ9eEiVgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8eaea1e-1fad-48c4-f403-dde3414ed43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 3: Gradient Clipping ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Task 3: Gradient Clipping ---\")\n",
        "rnn = nn.RNN(input_size=1, hidden_size=1, batch_first=True)\n",
        "optimizer = optim.SGD(rnn.parameters(), lr=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cHRoraHCiVgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f14e8da-43a8-4182-d431-bb61ce1ac1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.780032\n",
            "Epoch 2, Loss: 0.772807\n",
            "Epoch 3, Loss: 0.206455\n",
            "Epoch 4, Loss: 0.656150\n",
            "Epoch 5, Loss: 0.044063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output, _ = rnn(x)                   # forward pass\n",
        "    y_pred = output[:, -1, :]            # take last output\n",
        "    loss = criterion(y_pred, target.float().unsqueeze(1))  # match shapes\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0) # If that norm is greater than max_norm (1.0 here), it rescales all gradients proportionally so the total norm becomes 1.0.\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyzZbkLMkvB"
      },
      "source": [
        "------------------------------<br>\n",
        "Task 4: Manual Forward Pass <br>\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlcGh9oLMkvC"
      },
      "outputs": [],
      "source": [
        "def task4_manual_forward_pass():\n",
        "    \"\"\"\n",
        "    Compute a forward pass manually (hidden and output state) for a small LSTM using the activation functions in the formula.\n",
        "    Input sequence length T=3, input size=2, hidden size=2\n",
        "    \"\"\"\n",
        "    x_seq = [np.array([0.5, -1.0]),\n",
        "             np.array([1.0, 0.0]),\n",
        "             np.array([-0.5, 0.5])]\n",
        "    h_prev = np.zeros(2)\n",
        "    # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWo8cJeZiVgI"
      },
      "source": [
        "====================================================<br>\n",
        "Preprocess IMDB dataset<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeIIf1ziiVgI"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Loading IMDB dataset ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grfYypcBiVgK"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 5 — Implement LSTM Sentiment Classifier on the IMDB dataset<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnS_7rGyiVgL"
      },
      "outputs": [],
      "source": [
        "class GRUClassifier(nn.Module):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "====================================================<br>\n",
        "TASK 6 — Swap LSTM with GRU and repeat Task 4<br>\n",
        "===================================================="
      ],
      "metadata": {
        "id": "P8e_YYtTkfoY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ERaPf2BiVgN"
      },
      "outputs": [],
      "source": [
        "class GRUClassifier(nn.Module):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25aw0ZzHkJ_A"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 7 <br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbJO5GwBkJ_B"
      },
      "source": [
        "Compare loss curves for the LSTM and GRU classifiers. Which performs better and why?<br>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}